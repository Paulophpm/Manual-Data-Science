# Breve Guia para Projetos de An√°lise e Ci√™ncia de Dados
Esse √© um manual simples e pr√°tico para guiar o andamento de projetos de an√°lise e ci√™ncia de dados utilizando, a fim de deixar o processo de an√°lise mais organizado aumentando a produtividade para focar mais nos processos no seu c√≥digo, disponibilizando alguns templates para ser utilizados como base e algumas dicas de c√≥digo para deixar o seu projeto mais clean e objetivo.

Todos as dicas e templates foram criados do zeros e s√£o os que utilizo em todos os meu projetos de an√°lise de dados

Templates;

- [Trello - Gerencimento de Tarefas](https://trello.com/b/Cb7Rzdex/data-analysis);
- [Figma - Pipeline de Dados](https://www.figma.com/file/a6JA3XC5vdV7VTSBWreZ7H/Template?node-id=0%3A1&t=EfKPDgftA4OTLkSp-1);
[Figma - Mocukp]


## 1 - Origem dos Dados;

<aside>
üí° Verificar os dados que s√£o necess√°rios para come√ßar a construir o projetos e verificar algumas de suas propriedades. Assim j√° ter√° uma certa no√ß√£o de alguns processamentos que v√£o ser necess√°rios ser realizado no decorrer do projeto.
</aside>

Algumas fontes de dados j√° possuem Metadados, que traz basicamente todas as propriedades do conjunto de dados em quest√£o. Como o tipo de objeto de cada coluna, tamanho do arquivo, tamanho do dataset‚Ä¶ o que j√° facilita bastante a nossa vida.

Por√©m nem todas as fontes v√£o ter de forma t√£o simples e f√°cil esses metadados, por isso gosto sempre de criar um resumo com algumas propriedades b√°sicas do conjunto de dados, o que me d√° um no√ß√£o do que vou precisar realizar e assim que consigo ter uma melhor dimens√£o das an√°lises.

### 1.1 - Metadados
Ao pensar na origem dos dados, √© importante entender se o seu projeto utuliza mais de uma origem de fonte de dados, e qual √© o processo para acessar essa fonte

- Localiza√ß√£o da fonte de dados (Sharepoint, Web, Postgree, Kaggle, MongoDB‚Ä¶);
- Verificar credenciais para acesso dos dados (Organizacional, Chave de acesso...);
- Atualiza√ß√µes dos dados (em quanto tempo esses dados s√£o atualizados?)

### 1.1 - Metadados

- Verificar a Extens√£o dos Dados (csv, json, txt‚Ä¶);
- Localiza√ß√£o da Fonte de Dados;
- Credenciais para acesso dos dados;
- Formato do arquivo;
- Verificar encoding;
- Delimitador;
- Tamaho dos dados;
- Conex√£o com os dados

### 1.2 - Pipeline de Dados

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cf1cd3bf-8305-4b5e-9b58-4f776624d9a5/Untitled.png)

## Git

## 2. Carregamento dos Dados
<aside>
üí° Carregar os dados da forma correta √© imprescind√≠vel para evitar poss√≠vel problemas durante o decorrer do seu projeto, assim como carregar apenas os pacotes que forem necess√°rio ou at√© mesmo apenas os m√≥dulos dos pacotes que forem efetivamente utilizados. Dessa forma √© poss√≠vel deixar o seu projeto mais leve, tanto de mem√≥ria quanto de processamento.
</aside>

### 2.1 Importa√ß√£o

```python
# importas os pacotes que geralmente s√£o mais utilizados
# lembrando de carregar apenas o que for utilizar
import pandas as pd 
import numpy as np
import matplotlib.pyplot as mlp
import seaborn as sns
```

### 2.2 Configurando as importa√ß√µes

```python
# Configura√ß√£o dos plots do NumPy
np.set_printoptions(suppress = True, linewidth = 200, precision = 2)
# suppress = 
# linewidth = 
# precision = quantidade de casas decimais
# configura√ß√£o de impress√£o do Pandas
pd.
```

```python
# Par√¢metros de configura√ß√£o Matplotlib
from matplotlib import rcParams

rcParams['figure.figsize'] = 12, 4 # tamanho da grade da figura a ser criada
rcParams['lines.linewidth'] = 3 # quantidade de linhas
rcParams['xtick.labelsize'] = 'x-large' # largura do eixo x √© o total da tela dispon√≠vel
rcParams['ytick.labelsize'] = 'x-large' # largura do exio y √© o total da tela dispon√≠vel
```
### 2.3 Checkpoint

Fun√ß√£o para salvar resultados intermedi√°rios que est√° sendo realizado dentro do projeto

```python
# Fun√ß√£o chekpoint para salvar etapas do andamento do projeto em numpy
import numpy as np

def checkpoint(file_name, checkpoint_header, checkpoint_data):
    np.savez(file_name, header = checkpoint_header, data = checkpoint_data)
    checkpoint_variable = np.load(file_name + ".npz") # testar se funciona com csv
    return(checkpoint_variable)
```

```python
import pandas as pd

def checkpoint(file_name, checkpoint_header, checkpoint_data):
	pd.save(
	return(checkpoint_variable)

```

## Carregando Dataset

<aside>
üí° Carregar o dataset de acordo com a extens√£o do artigo, e garantir que seja todos os dados sejam devidamente carregados;
</aside>

Pontos a Verificar:

- Encoding;
- Delimitador;
- Escape de Caractere;
- Header (Cabe√ßalho);
- Concatenar tabelas (recomendo ap√≥s a limpeza);

## 3 - Data Wrangling/Munging

<aside>
üí° Data Wrangling (ou Data Munging) √© o processo de limpeza, transforma√ß√£o e organiza√ß√£o dos dados brutos para torn√°-los mais adequados para an√°lise de dados. Isso pode incluir a remo√ß√£o de dados ausentes, a corre√ß√£o de dados incorretos ou inconsistentes, a combina√ß√£o de v√°rias fontes de dados em um √∫nico conjunto de dados, entre outras tarefas de prepara√ß√£o de dados..

</aside>

### 3.1 - Formata√ß√£o de Valores

Formatar corretamente, ou verificar se as colunas do seu dataset est√£o devidamente configuradas no formato correto para que seja analisadas de formar correta em breve. 

Mesmo que alguns valores j√° venham formatado como valores num√©ricos, √© importante verificar se por acaso eles n√£o foram carregados em um tipo num√©rico diferetnte

Uma boa pr√°tica √©, se poss√≠vel separa o dataset em quest√£o em dois, um com valores num√©ricos e outro com valores strings, pois dessa forma poder√° ser mais f√°cil tratar as colunas j√° que vai ser poss√≠vel tratar mais de uma coluna de uma √∫nica vez.

<aside>
üí° Separar o dataset em dados num√©ricos e dados do tipo string para facilitar a limpeza dos dados em quest√£o

</aside>

- Formata√ß√£o dos valores Num√©ricos;
    - Data/Hora;
    - Moeda (qual regi√£o)
    - CEP;
    - Latitude;
    - Tipo num√©rico (Double, int,);

No caso de caracteres do tipo string, √© importante observar se todos os caracteres foram devidamente carregados com o encoding correto, e se h√° algum escape de caractere.

- Formata√ß√£o de Caracteres Strings:
    - Endere√ßo por extenso;
    - Cidade;
    - Estado;

# 4 - An√°lise Explorat√≥ria dos Dados - EDA

<aside>
üí° An√°lise explorat√≥ria √© uma abordagem inicial de an√°lise de dados que busca entender as principais caracter√≠sticas e padr√µes dos dados antes de aplicar t√©cnicas mais avan√ßadas de modelagem ou estat√≠stica inferencial. A an√°lise explorat√≥ria √© geralmente realizada no in√≠cio de um projeto de an√°lise de dados e pode incluir:

</aside>

z - escore (outliers)

Desvio Padr√£o

Media

Z-escore

<aside>
üí° O Z-score tem com objetivo identificar os valores que est√£o acima do desvio padr√£o dos dados que est√£o sendo observados

</aside>

```python
from scipy import stats
# Loop por cada vari√°vel num√©rica
for col in nums2:
    
    # Calcula o z-score absoluto
    zscore = abs(stats.zscore(df[col])) 
    
    # Mant√©m valores com menos de 3 z-score absoluto
    registros = (zscore < 3) & registros
```

### Limpeza dos Dados:

- Verificar dados duplicados
    
    Uma coisa √© verificar se existem valores duplicados, outra √© se existem linhas duplicadas, para cada tipo de situa√ß√£o √© necess√°rio entender o contexto dos dados para 
    
- Verificar dados Nan (Ausentes)
- Verificar caracteres especiais
- Decomposi√ß√£o ou Composi√ß√£o de Colunas

### Tratamento de Valores Ausente / Nan

<aside>
üí° N√£o √© s√≥ porque um valor de uma base est√° ausente, significa que esteja errado. Valor Nulo √© diferente de ausente

</aside>

Verificar a quantidade de valores que est√£o ausente, e de que formas eles est√£o ausentes, por colunas ou por linha, verificar se na verdade h√° algum erro no carregamento dos dados ou encoding utilizado

EDA

# 5 - Engenharia de Atributos

<aside>
üí° A Engenharia de atributos √© a etapa de cria√ß√£o de novas vari√°veis ou atributos (tamb√©m chamados de features) a partir dos dados brutos para melhorar o desempenho dos modelos de Machine Learning. Essa etapa envolve a sele√ß√£o e transforma√ß√£o de vari√°veis existentes, bem como a cria√ß√£o de novas vari√°veis que podem ser mais relevantes para o modelo. Isso pode incluir a cria√ß√£o de vari√°veis de tempo, vari√°veis categ√≥ricas, vari√°veis num√©ricas ou combina√ß√µes de v√°rias vari√°veis.

</aside>

```bash
acme deploy --prod
```

# Pr√©-Processamento de Dados;

<aside>
üí° O Pr√©-Processamento de dados √© utilizando sempre que for necess√°rio utilizar machine learning, transformando simplificando todas as vari√°veis. Modificando os dados, sem perder a informa√ß√£o

</aside>

### Label Encoding - Strings

Label Encoding (codifica√ß√£o de r√≥tulos) √© uma t√©cnica de codifica√ß√£o para lidar com vari√°veis categ√≥ricas. Nesta t√©cnica, a cada r√≥tulo √© atribu√≠do um n√∫mero inteiro exclusivo com base na ordem alfab√©tica. Na codifica√ß√£o de r√≥tulos em Python, substitu√≠mos o valor categ√≥rico por um valor num√©rico entre 0 e o n√∫mero de classes menos 1. Se o valor da vari√°vel categ√≥rica contiver 5 classes distintas usamos (0, 1, 2, 3 e 4). Observe que os dados s√£o modificados, mas sem perder a informa√ß√£o que eles representam. Isso pode ser feito de maneira manual (usando dicion√°rios em Python) ou com o algoritmo Label Encoder do pacote Scikit-Learn.

### One Hot Encoding

Nessa abordagem, para cada categoria de um recurso, criamos uma nova coluna (√†s vezes chamada de vari√°vel fict√≠cia) com codifica√ß√£o bin√°ria (0 ou 1) para indicar se uma determinada linha pertence a essa categoria.Vamos considerar aimagem abaixo. Observe que a vari√°vel Color possui 3 categorias (Red, Yellow e Green). 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b8a3174d-e4b8-4398-9869-76f018dcb145/Untitled.png)

Aplicando One-Hot Encoding3 novas vari√°veis s√£o criadas, sendo o valor 1 quando a ocorr√™ncia daquela cor e 0 quando n√£o h√° ocorr√™ncia.Uma potencial desvantagem desse m√©todo √© um aumento significativo na dimensionalidade do conjunto de dados (que √© chamado de Curse of Dimensionality).Ou seja, a codifica√ß√£o one-hot √© o fato de estarmos criando colunas adicionais, uma para cada valor exclusivo no conjunto do atributo categ√≥rico que gostar√≠amos de codificar. 

Portanto, se tivermos um atributo categ√≥rico que contenha, digamos, 1.000 valores exclusivos, essa codifica√ß√£o one-hot gerar√° 1.000 novos atributos adicionais e isso n√£o √© desej√°vel. Para simplificar, a codifica√ß√£o one-hot √© uma ferramenta bastante poderosa, mas s√≥ √© aplic√°vel para dados categ√≥ricos que possuem um n√∫mero baixo de valores exclusivos. One-Hot Encoding √© o processo de cria√ß√£o de vari√°veis fict√≠cias.

